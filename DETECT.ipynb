{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8091ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import cv2\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Define the architecture here\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.final_conv = nn.Conv2d(64, 2, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass here\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "segmentation_model = UNet()\n",
    "segmentation_model.load_state_dict(torch.load('model (1).pt', map_location=torch.device('cpu')))\n",
    "segmentation_model.eval()\n",
    "\n",
    "# Load the Vision Transformer classification model\n",
    "model_checkpoint = 'google/vit-base-patch16-224-in21k'\n",
    "\n",
    "class VIT(nn.Module):\n",
    "    def __init__(self, config=ViTConfig(), num_labels=2, model_checkpoint='google/vit-base-patch16-224-in21k'):\n",
    "        super(VIT, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.pooler_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)['last_hidden_state']\n",
    "        x = self.pooler_activation(self.pooler(x[:, 0, :]))\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n",
    "classification_model = VIT()\n",
    "classification_model.load_state_dict(torch.load('weed_detection_model.pth', map_location=torch.device('cpu')))\n",
    "classification_model.eval()\n",
    "\n",
    "class_names = [\"non-weed\", \"weed-images\"]\n",
    "\n",
    "# Define the transformations to apply to the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    \n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path =  'C:\\\\Users\\\\selman\\\\Downloads\\\\healthy_07\\\\healthy_02\\\\fbbb6a37-333a-4a09-a048-d994f1cb5eb0.JPG'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "# Perform classification using the Vision Transformer model\n",
    "# Perform classification using the Vision Transformer model\n",
    "# Perform classification using the Vision Transformer model\n",
    "with torch.no_grad():\n",
    "    classification_output = classification_model(image_tensor)\n",
    "_, predicted_classes = torch.topk(classification_output, k=2, dim=1)\n",
    "confidences = torch.softmax(classification_output, dim=1)[0, predicted_classes]\n",
    "\n",
    "# Extract the top predicted class and its confidence\n",
    "top_predicted_class = predicted_classes[0, 0].item()\n",
    "top_predicted_class_name = class_names[top_predicted_class]\n",
    "top_confidence = confidences[0, 0].item()\n",
    "\n",
    "# Check if both weed and non-weed classes are present\n",
    "if 0 in predicted_classes and 1 in predicted_classes:\n",
    "    second_predicted_class = predicted_classes[0, 1].item()\n",
    "    second_predicted_class_name = class_names[second_predicted_class]\n",
    "    second_confidence = confidences[0, 1].item()\n",
    "else:\n",
    "    second_predicted_class = None\n",
    "    second_predicted_class_name = None\n",
    "    second_confidence = None\n",
    "\n",
    "# Print the predicted classes and confidences\n",
    "print(\"Top Predicted Class:\", top_predicted_class_name)\n",
    "print(\"Top Confidence:\", top_confidence)\n",
    "if second_predicted_class is not None:\n",
    "    print(\"Second Predicted Class:\", second_predicted_class_name)\n",
    "    print(\"Second Confidence:\", second_confidence)\n",
    "\n",
    "# Perform segmentation using the U-Net model\n",
    "with torch.no_grad():\n",
    "    segmentation_output = segmentation_model(image_tensor)\n",
    "\n",
    "# Process the segmentation output\n",
    "# Convert the segmentation output to a binary mask\n",
    "binary_mask = (segmentation_output > 0.5).float()\n",
    "binary_mask = binary_mask.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Create a mask with the blue color for segmented parts\n",
    "blue_color = np.array([0, 0, 255], dtype=np.uint8)\n",
    "\n",
    "# Overlay the segmentation mask on the original image\n",
    "segmented_image = image_tensor.squeeze().permute(1, 2, 0)\n",
    "segmented_image = segmented_image.cpu().numpy()\n",
    "segmented_image[binary_mask == 1] = blue_color\n",
    "\n",
    "# Convert the segmented image back to PIL format\n",
    "segmented_image = Image.fromarray(segmented_image.astype(np.uint8))\n",
    "\n",
    "\n",
    "# Display the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(image)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Input Image')\n",
    "\n",
    "ax2.imshow(segmented_image)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Segmented Image')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the predicted classes and confidences\n",
    "print(\"Predicted Classes:\", predicted_classes)\n",
    "print(\"Confidences:\", confidences)\n",
    "# Generate classification report\n",
    "target_names = class_names\n",
    "classification_rep = classification_report([top_predicted_class], [top_predicted_class], labels=[0, 1], target_names=target_names,zero_division=0)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Create a list of colors corresponding to each predicted class\n",
    "colors = ['red', 'green', 'blue', 'yellow', 'orange']  # Add more colors if needed\n",
    "\n",
    "# Plot the 2D histogram with different colors\n",
    "plt.hist2d(predicted_classes.squeeze().tolist(), confidences.squeeze().tolist(), bins=[len(class_names), 10], cmap='viridis')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Predicted Classes')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Predicted Classes vs. Confidences')\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# Add a legend for the colors\n",
    "legend_labels = class_names\n",
    "plt.legend(legend_labels)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
