{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "9e8200fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the segmentation model and classification model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Define the architecture here\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.final_conv = nn.Conv2d(64, 2, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass here\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "segmentation_model = UNet()\n",
    "segmentation_model.load_state_dict(torch.load('model (1).pt', map_location=torch.device('cpu')))\n",
    "segmentation_model.eval()\n",
    "class VIT(nn.Module):\n",
    "    def __init__(self, config=ViTConfig(), num_labels=2, model_checkpoint='google/vit-base-patch16-224-in21k'):\n",
    "        super(VIT, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.pooler_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)['last_hidden_state']\n",
    "        x = self.pooler_activation(self.pooler(x[:, 0, :]))\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "classification_model = VIT()\n",
    "classification_model.load_state_dict(torch.load('weed_detection_model.pth', map_location=torch.device('cpu')))\n",
    "classification_model.eval()\n",
    "\n",
    "# Define the class names\n",
    "class_names = [\"non-weed\", \"weed-images\"]\n",
    "\n",
    "# Define the transformations to apply to the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Define the function to preprocess the input image\n",
    "def preprocess_image(image):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "# Define the function to perform the prediction\n",
    "def predict_image(image):\n",
    "    # Preprocess the image\n",
    "    image_tensor = preprocess_image(image)\n",
    "\n",
    "    # Perform classification using the Vision Transformer model\n",
    "    with torch.no_grad():\n",
    "        classification_output = classification_model(image_tensor)\n",
    "    _, predicted_classes = torch.topk(classification_output, k=2, dim=1)\n",
    "    confidences = torch.softmax(classification_output, dim=1)[0, predicted_classes]\n",
    "\n",
    "    # Extract the top predicted class and its confidence\n",
    "    top_predicted_class = predicted_classes[0, 0].item()\n",
    "    top_predicted_class_name = class_names[top_predicted_class]\n",
    "    top_confidence = confidences[0, 0].item()\n",
    "\n",
    "    # Check if both weed and non-weed classes are present\n",
    "    if 0 in predicted_classes and 1 in predicted_classes:\n",
    "        second_predicted_class = predicted_classes[0, 1].item()\n",
    "        second_predicted_class_name = class_names[second_predicted_class]\n",
    "        second_confidence = confidences[0, 1].item()\n",
    "    else:\n",
    "        second_predicted_class = None\n",
    "        second_predicted_class_name = None\n",
    "        second_confidence = None\n",
    "\n",
    "    # Perform segmentation using the U-Net model\n",
    "    with torch.no_grad():\n",
    "        segmentation_output = segmentation_model(image_tensor)\n",
    "\n",
    "    # Process the segmentation output\n",
    "    binary_mask = (segmentation_output > 0.5).float()\n",
    "    binary_mask = binary_mask.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    blue_color = np.array([0, 0, 255], dtype=np.uint8)\n",
    "    segmented_image = image_tensor.squeeze().permute(1, 2, 0)\n",
    "    segmented_image = segmented_image.cpu().numpy()\n",
    "    segmented_image[binary_mask == 1] = blue_color\n",
    "    segmented_image = Image.fromarray(segmented_image.astype(np.uint8))\n",
    "\n",
    "    # Return the predicted classes, confidences, and segmented image\n",
    "    return top_predicted_class_name, top_confidence, second_predicted_class_name, second_confidence, segmented_image\n",
    "\n",
    "# Define the inputs and outputs for the gradio interface\n",
    "inputs = gr.Image()\n",
    "outputs = [\n",
    "    gr.Textbox(label=\"Top Predicted Class\"),\n",
    "    gr.Textbox(label=\"Top Confidence\"),\n",
    "    gr.Textbox(label=\"Second Predicted Class\"),\n",
    "    gr.Textbox(label=\"Second Confidence\"),\n",
    "    gr.Image(label=\"Segmented Image\")\n",
    "]\n",
    "\n",
    "# Create the gradio interface\n",
    "gr.Interface(fn=predict_image, inputs=inputs, outputs=outputs).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
